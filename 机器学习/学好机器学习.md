### 学习整体思路

![sadsad](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/20210715134907.jpg)



首先需要3个方面的准备。

**其实总结来看的话，学机器学习就两步，一个是是掌握经典算法原理(但是需要其他学科知识的支撑)，然后是熟练运用Python工具包进行建模实战。**

我现在主要在做的还是理解原理。



#### 数学基础

机器学习之所以相对于其他开发工作更有门槛，根本原因就在于数学。

每一个算法，要在训练集上最大程度拟合同时又保证泛化能力，需要不断分析结果和数据，调优参数，这需要我们对数据分布和模型底层的数学原理有一定的理解。

所幸的是如果只是想合理应用机器学习，而不是做相关方向高精尖的research，需要的数学知识啃一啃还是基本能理解下来的。

基本所有常见机器学习算法需要的数学基础，都集中在**微积分、线性代数和概率与统计**当中。

##### 微积分

* 微分的计算及其几何、物理含义，是机器学习中大多数算法的求解过程的核心。比如算法中运用到梯度下降法、牛顿法等。如果对其几何意义有充分的理解，就能理解“梯度下降是用平面来逼近局部，牛顿法是用曲面逼近局部”，能够更好地理解运用这样的方法。
* 凸优化和条件最优化 的相关知识在算法中的应用随处可见，如果能有系统的学习将使得你对算法的认识达到一个新高度。

##### 线性代数

* 大多数机器学习的算法要应用起来，依赖于高效的计算，这种场景下，程序员GG们习惯的多层for循环通常就行不通了，而大多数的循环操作可转化成矩阵之间的乘法运算，这就和线性代数有莫大的关系了
* 向量的内积运算更是随处可见。
* 矩阵乘法与分解在机器学习的主成分分析（PCA）和奇异值分解（SVD） 等部分呈现刷屏状地出现。

##### 概率与统计

从广义来说，机器学习在做的很多事情，和统计层面数据分析和发掘隐藏的模式，是非常类似的。

* 极大似然思想、贝叶斯模型 是理论基础，朴素贝叶斯(Na?ve Bayes )、语言模型(N-gram)、隐马尔科夫（HMM）、隐变量混合概率模型是他们的高级形态。

* 常见分布如高斯分布是混合高斯模型(GMM)等的基础。



#### 典型算法

绝大多数问题用典型机器学习算法都能解决，粗略列举如下。

* 处理**分类问题**的常用算法包括：逻辑回归(工业界最常用)，支持向量机，随机森林，朴素贝叶斯(NLP中常用)，深度神经网络(视频、图片、语音等多媒体数据中使用)。
* 处理**回归问题**的常用算法包括：线性回归，普通最小二乘回归（Ordinary Least Squares Regression），逐步回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）处理聚类问题的常用算法包括：K均值（K-means），基于密度聚类，LDA等等。

* 降维的常用算法包括：主成分分析（PCA）,奇异值分解（SVD） 等。
* 推荐系统的常用算法：协同过滤算法
* 模型融合(model ensemble)和提升(boosting)的算法包括：bagging，adaboost，GBDT，GBRT
* 其他很重要的算法包括：EM算法等等。



我们多插一句，机器学习里所说的“算法”与程序员所说的“数据结构与算法分析”里的“算法”略有区别。前者更关注**结果数据的召回率、精确度、准确性等方面**，后者更关注执行过程的时间复杂度、空间复杂度等方面 。当然，实际机器学习问题中，对效率和资源占用的考量是不可或缺的。



#### 编程语言

对初学者而言，Python和R语言是很好的入门语言，很容易上手，同时又活跃的社区支持，丰富的工具包帮助我们完成想法。相对而言，似乎计算机相关的同学用Python多一些，而数学统计出身的同学更喜欢R一些。

附：我个人目前觉得先用Python就够了，暂时就先不用再学一门语言了



Pycharm是一款Python语言编译器，大家很推崇。

python有着全品类的数据科学工具，从数据获取、数据清洗到整合各种算法都做得非常全面。

* 网页爬虫：scrapy
* 数据挖掘
  * pandas：模拟R，进行数据浏览与预处理
  * numpy：数组运算
  * scipy：高效的科学运算
  * matplotlib：非常方便的数据可视化工具
* 机器学习
  * scikit-learn：远近闻名的机器学习package。未必是最高效的，但是接口真心封装得好，几乎所有的机器学习算法输入输出部分格式都一致。而它的支持文档甚至可以直接当做教程来学习，非常用心。对于不是非常高纬度、高量级的数据，scikit-learn胜任得非常好(有兴趣可以看看sklearn的源码，也很有意思)。
  * libsvm：高效率的svm模型实现(了解一下很有好处，libsvm的系数数据输入格式，在各处都非常常见)
  * keras/TensorFlow：对深度学习感兴趣的同学，也能很方便地搭建自己的神经网络了。
* 自然语言处理
  * nltk：自然语言处理的相关功能做得非常全面，有典型语料库，而且上手也非常容易。
* 交互式环境
  * ipython notebook：能直接打通数据到结果的通道，方便至极。强力推荐。



后面的补充：对于只习惯windows的同学，推荐**anaconda**，一步到位安装完python的全品类数据科学工具包。



R最大的优势是开源社区，聚集了非常多功能强大可直接使用的包，绝大多数的机器学习算法在R中都有完善的包可直接使用，同时文档也非常齐全。常见的package包括：RGtk2, pmml, colorspace, ada, amap, arules, biclust, cba, descr, doBy, e1071,  ellipse等等。另外，值得一提的是R的可视化效果做得非常不错，而这对于机器学习是非常有帮助的。

但由于我目前没有学习的想法，于是就先不看R的东西了。



#### 其他有帮助的东西

Octave：学习机器学习常用开源软件(功能与Matlab类似)





### 如何做一个完整的机器学习项目



#### 抽象成数学问题

- 明确问题是进行机器学习的第一步。**机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。**
- 这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

#### 获取数据

- 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
- **数据要有代表性，否则必然会过拟合。**
- 而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
- 而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。



#### 特征预处理与特征选择

- 良好的数据要能够提取出良好的特征才能真正发挥效力。

- 特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。**归一化、离散化、因子化、缺失值处理、去除共线性等**，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

- **筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。**这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

  

附：老师讲课提到未必用原本的特征，而是可以按照需要来自定义，后来给出的特征缩放什么的，其实就是在做这件事。总之就是方便更快让它收敛。



#### 训练模型与调优

* 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是**真正考验水平的是调整这些算法的（超）参数**，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。



#### 模型判断

如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。

- 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
- 误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
- 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。



#### 模型融合

**附：这个概念是什么意思呢？是上一步得到的新的模型和原本模型进行合并吗？不是替代？怎么合并呢？**

- 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
- 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。



#### 上线运行

* 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。



这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，最终还是要靠自己总结。



### 关于积累项目经验

初学机器学习可能有一个误区，就是一上来就陷入到对各种高大上算法的追逐当中。动不动就我能不能用深度学习去解决这个问题啊？我是不是要用boosting算法做一些模型融合啊？我一直持有一个观点，『脱离业务和数据的算法讨论是毫无意义的』。

实际上按我们的学习经验，从一个数据源开始，即使是用最传统，已经应用多年的机器学习算法，先完整地走完机器学习的整个工作流程，不断尝试各种算法深挖这些数据的价值，在运用过程中把数据、特征和算法搞透，真正积累出项目经验 才是最快、最靠谱的学习路径。

那如何获取数据和项目呢？一个捷径就是积极参加国内外各种数据挖掘竞赛，数据直接下载下来，按照竞赛的要求去不断优化，积累经验。国外的Kaggle和国内的DataCastle  以及阿里天池比赛都是很好的平台，你可以在上面获取真实的数据和数据科学家们一起学习和进行竞赛，尝试使用已经学过的所有知识来完成这个比赛本身也是一件很有乐趣的事情。和其他数据科学家的讨论能开阔视野，对机器学习算法有更深层次的认识。

有意思的是，有些平台，比如阿里天池比赛，甚至给出了从数据处理到模型训练到模型评估、可视化到模型融合增强的全部组件，你要做的事情只是参与比赛，获取数据，然后使用这些组件去实现自己的idea即可。具体内容可以参见阿里云机器学习文档。



### 自主学习能力



多几句嘴，这部分内容和机器学习本身没有关系，但是我们觉得这方面的能力对于任何一种新知识和技能的学习来说都是至关重要的。 自主学习能力提升后，意味着你能够跟据自己的情况，找到最合适的学习资料和最快学习成长路径。



#### 信息检索过滤与整合能力

对于初学者，绝大部分需要的知识通过网络就可以找到了。

google搜索引擎技巧——组合替换搜索关键词、站内搜索、学术文献搜索、PDF搜索等——都是必备的。

一个比较好的习惯是找到信息的原始出处，如个人站、公众号、博客、专业网站、书籍等等。这样就能够找到系统化、不失真的高质量信息。

百度搜到的技术类信息不够好，建议只作为补充搜索来用。各种搜索引擎都可以交叉着使用效果更好。

学会去常见的高质量信息源中搜索东西:stackoverflow（程序相关）、quora（高质量回答）、wikipedia（系统化知识，比某某百科不知道好太多）、知乎（中文、有料）、网盘搜索（免费资源一大把）等。

将搜集到的网页放到分类齐全的云端收藏夹里，并经常整理。这样无论在公司还是在家里，在电脑前还是在手机上，都能够找到自己喜欢的东西。

搜集到的文件、代码、电子书等等也放到云端网盘里，并经常整理。



#### 提炼与总结能力

经常作笔记，并总结自己学到的知识是成长的不二法门。其实主要的困难是懒，但是坚持之后总能发现知识的共性，就能少记一些东西，掌握得更多。

笔记建议放到云端笔记里，印象笔记、为知笔记都还不错。这样在坐地铁、排队等零碎的时间都能看到笔记并继续思考。



#### 提问与求助能力

机器学习的相关QQ群、论坛、社区一大堆。总有人知道你问题的答案。

但是大多数同学都很忙，没法像家庭教师那样手把手告诉你怎么做。

为了让回答者最快明白你的问题，最好该学会正确的问问题的方式:陈述清楚你的业务场景和业务需求是什么，有什么已知条件，在哪个具体的节点上遇到困难了，并做过哪些努力。

有一篇经典的文章告诉你怎样通过提问获得帮助：《提问的智慧》，强力推荐。 话锋犀利了些，但里面的干货还是很好的。

别人帮助你的可能性与你提问题的具体程度和重要性呈指数相关。



#### 分享的习惯

我们深信：“证明自己真的透彻理解一个知识，最好的方法，是给一个想了解这个内容的人，讲清楚这个内容。” 分享能够最充分地提升自己的学习水平。这也是我们坚持长期分享最重要的原因。

分享还有一个副产品，就是自己在求助的时候能够获得更多的帮助机会，这也非常重要。





### 相关资源

#### 数学理论知识

李航《统计学习方法》第二版

《矩阵论》西北工业大学出版社（主要了解矩阵求导法则、Hessian矩阵、范式等）

Stenphen_Boyd 《凸优化》（王书宁译） 



#### 专业理论知识

《机器学习理论导引》

周志华《机器学习》 配套的《南瓜书》

视频：吴恩达《机器学习》 



#### 实战

《深度学习》花书AI

《动手学深度学习》 



