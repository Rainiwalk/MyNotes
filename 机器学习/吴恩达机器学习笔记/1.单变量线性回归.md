理解：虽说是要解决单变量线性回归这个问题，但其实里面也给出了后续监督学习的一个讲述雏形，就是先将理论推导，然后给出损失函数等，最后尝试min的流程。



### 模型和代价函数Model and Cost Function



#### Linear Regression模型

首先监督学习，我们会有一个数据集，也被叫做训练集

![ads12](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807091627.png)



针对训练集我们要定义一些常用的符号表示.

![a3211](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807091725.png)

上面中：x表示的是输入变量，y为输出变量. (x,y)一个训练样本，加i表示是第i个训练样本.

我们还用X(大写)表示输入空间，Y表示输出空间.

在上面这个买房样例中，X = Y = 实数域R.

如果我有47行，则m(样本容量) = 47.





下面描述一下监督学习的工作流程.

![c111](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807092310.png)

我们向要待学习的算法提供训练集，比如说刚才的房价与面积.

学习算法的任务是输出一个假设函数h，h的作用是把房子大小作为输入变量，面对一个新的输入x去输出对应的预测y值. h就是x到y的一个映射.

注：你可能会疑惑中间的h为什么叫假设函数(**hypothesis**)，这个是有历史原因的。因为早期的机器学习是这么叫的，现在你觉得可能有些不太贴切，但也不必纠结，它只是一个名字而已.



那么紧接着我们就该想到，重要的就是怎样去表示这个h(因为我们要找的就是这个东西，一个映射关系).

![image-20210807093328480](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807093328.png)



这个给出一种假设表示，hθ(x) = θ0 + θ1x.  有时hθ(x)被缩写为h(x). ---- **在什么都不知道的情况下，我们可以随便假设.**

那尽管是随便假设，我们为什么假设它们是一种线性关系，h是一个线性函数呢？

有时候我们会想去拟合更加复杂的，比如非线性的函数，但由于这种线性情况是学习的基础，我们于是先从它开始，也就是先去拟合线性函数。然后在这个的基础上，最终在去拟合更复杂的模型。

让我们给上面这个模型一个名字，它就叫做：线性回归.

这个例子是一个一元线性回归，当然也叫做"单变量线性回归".   --- 单变量只是称呼单一变量的高大上的方式.

附：英文上可以是 linear regression(with one variable)，或 univariate linear regression.



**总结：**在监督学习这里，我们会有训练集，还有要使用的训练算法，训练算法将根据训练集训练产生一个假设函数h，它将接受输入并给出输出。重点也就在于怎样生成这样一个h，而且要他最拟合数据.

首先不能让它停留于概念，要先给它一个住宿的躯壳.也就是给它一些定义，我们好更好把握。

于是我们给出h(x)一个线性模型.其实在什么都知道的情况下，我们是可以随便假设的.为什么假设是线性模型呢？因为它是最简单的模型，我们从最简单的开始，慢慢再向复杂演化，这才是一个较好的认知过程.





----



#### 代价函数 Cost Function 定义

理解：**这里我们将定义代价函数，这有助于我们明白如何找到与数据相拟合的直线。**因为上面定义的也仅仅是个线性模型，其中参数不定，模型仍然无法使用.就是说我们只是把大体趋势假设下来了，具体要他与数据进行拟合，就需要想办法给模型中参数最合适的值，从而把模型具象化.



我们眼下已知，训练集和模型.

![image-20210807095134177](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807095134.png)

现在要解决的就是如何选择θ0和θ1这两个参数值.



如果我给他俩不同的值，模型就会有如下不同的表现.

![image-20210807095511715](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807095511.png)





在线性回归中，我们有一个训练集，我们把它绘制到图上变成点.

**我们要做的，就是给定两个参数值，来让假设函数表示的直线尽可能与图中的数据点进行拟合，像下面这样.**

![image-20210807095719792](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807095719.png)





那么怎么才能找到这两个参数呢？

让我们给出标准的定义，在线性回归中，我们要解决的这个是一个**最小化问题**.

![image-20210807102936991](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807102937.png)

**对于怎样给出最拟合数据的直线，这就是数学上的表示.**也就是对于给定的输入x，我会得到一个模型预测值h(x)，还有训练集中的真实值y，我们想办法找到使二者差距达到最小(最接近)的两个参数，任务就算完成了.

目前这个式子还只是针对一个x，但我明显是希望模型对所有数据有最佳拟合，而不是单独某个数据.

所以要把上面这个公式应用到所有数据，于是就有了应用所有在求和.

![image-20210807103419336](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807103419.png)



而为了让这个表达式的数学意义变得容易理解一点，我们又给它乘上了1/m，这样也只会让它在数值大小上降低一点，如果不同参数都使用1/m这个常数项，那是不妨碍你进行彼此间的比较的.

然后再乘一个1/2，还是有上面的原因，乘以常数对比较没有影响.此外，增加一个1/2也是为了方便未来可能进行优化，因为优化时涉及到求导，1/2可以抵消掉平方求导.

理解：其实选用平方而不是绝对值，也有方便后续求导的考虑.



所以最后公式就变成了这样.

![image-20210807104054746](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807104054.png)

到了现在，简单说，我们正在把找到根据数据找到最拟合参数的问题转换为：找到能使我的训练集中预测值与真实值差的平方的和的1/2m最小的θ0和θ1的值.

这样，就从原来的几何直观但不可计算转到了代数可计算可衡量的状态.



这就是我们线性回归的整体目标函数.

为了使它更明确，我们要改写这个函数。

按照惯例，**我们把上面右侧的式子定义成了Cost function代价函数，我们的目标就变成了找到minimize J(θ0,θ1)的两个参数.**

![image-20210807104547959](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807104548.png)



附：这个代价函数cost function也被叫做 squared error function 平方误差函数



理解：诶，发现联系啦，在回归模型的性能度量那里，度量方法"**均方误差**"，基本就是这个，只是没有1/2，差别不大.  这也就是说，**上面我们得到的这个代价函数，其实不只是应用在现在的单变量线性回归，对于回归问题好像都可以使用，是最常用的手段. 当然也有别的，后续会介绍.**





总结：我们现在有训练数据和我们预设的一个线性回归模型。我们希望的就是这个模型最拟合数据，画在图上就是每个样本点都要尽可能在线上，用数学的语言，就是每个样本预测值和真实值差距最小，应用到所有数据，要求就是整体差距和最小，而不同的参数控制的就是这个误差的大小. 我们把这个整体差距和前面乘上1/2m，因为是常数，所以不影响比较.把现在的公式定义为了代价函数.  我们找参数就是要找可以使这个函数值最小的两个参数，这两个参数也就是函数的自变量，输出值就是因变量.







---



#### 代价函数原理详解(1)



我们来更深度地认知代价函数.

为了更好地使代价函数J可视化，我们先使用一个简化地假设函数来辅助我们地理解.

![image-20210807110442204](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807110442.png)

可以理解为此时θ0 = 0，即删除了原假设中的常数项部分，但仍然保持线性. --- 此时相当与模型又确定了一点，变成了过原点的直线.

此时我们minimizeJ，目的也就变成了找到对应的θ1这一个参数了.



使用这个简化的代价函数，我们讲更好理解它的概念.

我们来尝试画一下hθ(x)和J(θ1)的图像.

![image-20210807111221281](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807111221.png)

hθ(x) = θ1x，如果θ1 = 1，就会化成左图，然后根据左图就可以算出对应的J(1) = 0，画在右图.



画了很多以后，就得到了右边一副图像.

![image-20210807112109694](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807112109.png)

我们的目标是什么来的？

答曰：找到使J(θ1)最小的那个参数θ1。很明显可以从图中看出，当θ1=1时，J最小。那么对于现在的这个数据集，我们就已经完美拟合它了.



总结：这次我们把原本假设模型简化，变成只有一个参数，这样我们更好理解更好画图.于是我们分别画出了hθ(x)和J(θ1)，算是从几何直观上理解了这些东西.





---



#### 代价函数原理讲解(2)

现在我们将回到那个最初的双参线性模型了.

![image-20210807112743738](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807112743.png)

理解：突然意识到数学的用途，本来在言语上所说的拟合，我们现在已经能把它转换成上面这些很有条理的公式了.



还是尝试直观画出两幅图像.

![image-20210807113042520](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807113042.png)

但是，由于这次有两个参数，而且还需要一个维度表示J的值，所以图像总共需要三个维度.

此时，你会得到类似这样的代价函数，这是一个3D曲面图，轴标为θ0和θ1，它们对应的值是J，也就是竖轴的表示(看高度)。

![image-20210807113217285](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807113217.png)



但为了在接下来的讲解中更好地展现图形，我们决定不用这个，我们要使用等高线图(contour plots / contour figures)来进行展示.

![image-20210807113658531](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807113658.png)

在右图等高线图中，其轴为θ0和θ1.

图中的每个椭圆形，表示的都是J(θ0,θ1)相等的点，也就是说下面标注的这三点J一样.

![image-20210807113904087](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807113904.png)

理解：感觉就是把那个3D图形用水平面去切的结果.

**而且，在图中最积聚的地方，就是我们要找的最小值点(对于为什么是最小值点，其实你想一下就可以知道，我可以让直线无限地离谱，就是误差可以拉得无限大，但是最小却只有一种)**



我们尝试找到图中一个点，他会对应两个参数值，但是明显它在左图中表示拟合得并不好.

![image-20210807114415567](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807114415.png)

但如果选这个点，可能就十分接近拟合最佳状态了.

![image-20210807114557950](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210807114558.png)

  

我们最想要的是一个高效的算法，来自动寻找代价函数J的最小值对应的两个参数.

我们不希望编写出一个软件，可以画出这个图，然后再去手动读取参数.

如果这里你还可以逞强说自己可以去找，那么后续我们也可能会遇到一些很难可视化的图像，此时我们就需要利用软件来帮我们自动寻找了.



接下来我们就将学习一种可以自动找到使函数J最小的参数的算法



总结：和上一节目的一致，就是让你对两个函数有一个直观的理解.而且还介绍了等高线图，它更方便对双参函数进行展示.







---



### 参数学习 --- 梯度下降法Gradient Descent





