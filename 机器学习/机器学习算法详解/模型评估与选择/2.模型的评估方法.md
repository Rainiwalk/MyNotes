### 评估思路

我们之前已经说了，泛化误差是在整个数据全集上的误差，如果它的值最小，那这个模型就是我们需要的。

**所以我们的思路就是通过实验测试，对模型的泛化误差进行评估，选出泛化误差最小的模型。**

但实际上，数据全集在绝大部分情况下我们拿不到。因为未来会加入什么东西，它们的信息我们根本就不知道，永远是未知的。

理解：所以可以说，这个泛化误差和那个期望风险有些异曲同工，都是很理论上的东西



但是我们可以用**测试误差(Testing Error)**对泛化误差进行近似，然后用它来对模型进行评估。

这就引入了数据集的处理方法，比如我现在有一个已知的数据集，那我们可以把它80%用来作为训练集来训练，剩余20%作为测试集。

那80%用来生成模型，后来用20%进行测试，就可以得到测试误差，就用它来衡量。



![image-20210802113806529](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210802113806.png)



划分要注意：

* 测试集和训练集尽可能互斥(不一样)

  如果你考试考的题目都是平常的例题，那对能力的考查就是无效的了

* 测试集和训练集独立同分布

  独立：事件之间没有依赖关系，两个集中的数据没有依赖关系

  同分布：测试集和训练集最好数据分布是一致的

  举个例子，我们来预测身高，父亲和孩子的信息，训练集都来自于中国，但测试集都来自于冰岛，这就会有问题。因为中国人身高分布的特征和冰岛人相比一定是有明显区别的。

  也就是说你的测试集和训练集应该是会有一些宏观的相似

  



### 评估方法



#### 留出法

**留出法(Hold-out)**：将已知数据集分成两个互斥的部分，其中一部分用来训练模型，另一部分用来测试模型，评估其误差，作为泛化误差的估计。



**注意**

* 两个数据集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入人为的误差

  比如这样一个人群，上面男性下面女性

  ![image-20210802115801964](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210802115802.png)

  你去划分的话，两部分男女占比都一样可能表现会更好。不然，比如买护肤品，男性女性可能就会表现出很大的差异，给训练带来问题。

  为了保持样本的类别比例相似，最好采用**分层采样(Stratified Sampleing)**。
  
  

* 数据分割存在多种形式会导致不同的训练集、测试集划分，单次留出法结果往往存在偶然性，其稳定性较差，通常会进行若干次随机划分、重复实验评估平均值作为评估结果
* 数据集拆分成两部分，每部分的规模设置也会影响评估结果，测试、训练的比例通常为7:3、8:2等

  



#### 交叉验证法

**交叉验证法(Cross Validation，CV)**：将数据集划分为k个大小相似的互斥的数据子集，子集数据尽可能保持数据分布的一致性(分层采样)，每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。

该验证方法也称作 k折交叉验证( k-fold Cross Validation)。

使用不同的划分，重复p次，称为p次k折交叉验证。



样例

![image-20210802120932011](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210802120932.png)

 10折交叉验证，最后10次再得到一个评估均值，用这个来反映模型好不好的状况





#### 留一法

**留一法(Leave-One-Out，LOO)**：是k折交叉验证的特殊形式，将数据集分成两个，其中一个数据集记录条数为1，作为测试集使用，其余记录作为训练集训练模型。训练出的模型和使用全部数据集训练得到的模型接近，其评估结果比较准确。缺点是当数据集较大时，训练次数和计算规模较大。

理解：这划分也太极端了吧，不愧是特殊形式。

缺点就是训练量太大了，如果有10000条数据，那个9999个都要用于训练。



![image-20210802145730085](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210802145730.png)





#### 自助法

**自助法(Bootstrapping)/拔靴法**：是**一种产生样本的抽样方法**，其实质是有放回的随机抽样。即从已知数据集中随机抽取一条记录，然后将该记录放入测试集同时放回原数据集，继续下一次抽样，直到测试集中的数据条数满足要求。

理解：哦哦，这是从已知数据集随机抽东西放到测试集，因为是随机，所以其实是有可能重复的



理论

![image-20210802150450108](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210802150450.png)

这个通过一种概率学把数据集分成了两部分，36.8%的数据不会被选中就去做测试，被选中的是去做训练。



#### 几种方法适用场景



**留出法**

优点

* 实现简单、方便，在一定程度上能评估泛化误差
* 测试集和训练集分开，缓解了过拟合

缺点

* 一次拆分，评估结果偶然性大
* 数据被拆分后，用于训练、测试的数据更少了





**交叉验证法(留一法)**

优点

* k可以根据实际情况设置，充分利用了所有样本
* 多次划分，评估结果相对稳定

缺点

* 计算比较繁琐，需要进行k次训练和评估





**自助法**

优点

* 样本量较小时可以通过自助法产生多个自助样本集，且有约36.8%的测试样本
* 对于总体的理论分布没有要求

缺点

* 无放回抽样引入了额外的偏差

  重复记录的堆叠会提高它的权重，就相当于引入了额外的偏差





**几种方法的选择**

* 已知数据集数量充足时，通常采用留出法或者k折交叉验证法
* 对于已知数据集较小且难以有效划分训练集/测试集的时候，采用自助法
* 对于已知数据集较小且可以有效划分训练集/测试集的时候，采用留一法

