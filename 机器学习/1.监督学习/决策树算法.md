### 定义

决策树(DT，Decision Tree)



### 构建流程

![image-20210727174158775](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727174205.png)



确定终止条件：就是考虑什么时候这棵树算构建完成。



### 熟悉数据，确定目标

![image-20210727174624174](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727174624.png)

我们拿电脑购买记录来举例子。

自变量：年龄~信用等级

因变量(要预测的东西)：是否购买电脑

信息度量方式：就是你用什么东西来度量当前给定的这些信息，这里给了两种，即熵和基尼系数。**这个例子中我们选择"熵"。**

纯度：把记录分完支之后，当前的结果集里，是否购买电脑的类别是不是一样。如果全为是，那纯度就达标了，因为全都是了。

记录条数：当前待处理的数据集中的记录条数是不是低于某一个值，如果低于就结束

循环次数：不管纯度和记录条数，我就设置循环次数，只关心做了多少次循环。



#### 信息熵的解释

来源：信息论中的概念，由香农提出

作用：描述混乱程度的度量

性质：取值范围0~1，值越大，越混乱

计算公式：

![image-20210727175708518](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727175708.png)

其中，p为概率，这里就是概率乘以其对数，然后再求和。结果加负号。

一个计算样例。

![image-20210727175950460](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727175950.png)

理解：考虑随机拿出哪个，最不确定的就是第一个，后面至少都有一个朝向趋势。所以第一个就是最混乱的(最难预测)，第二个就是最有秩序的。通过运算值也可以看出。





#### 信息增益和特征选择

刚才说了熵是混乱性的度量，而**信息增益**是什么呢？

* 从一个状态到另一个状态信息的变化(这个举例里这个信息我们选择为熵)

* 信息确定性的增加
* 信息增益越大，对**确定性(混乱度的反面)**贡献越大



举例情况

![image-20210727181220168](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727181220.png)

收集到的信息有4行，每个带有两个特征，一个判断结果

最终...

![image-20210727181340779](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727181340.png)

1.如果没有考虑给定信息带有的特征，单纯就是纯看数据量，那么对应是水果和不是水果成为了计算熵的两项。概率都是2/4，因为4个样本分别有2。结果为1，则说明此时混乱程度最大。就是说随便来猜，出错概率是最大的。

2.考虑以颜色作为参考来判断是不是水果。先考虑各种颜色出现的概率，然后再乘上这种颜色对应是水果的熵的计算项。加了颜色属性，我们发现熵变成了0.689。

这也就是从一个状态到了另一个状态，信息发生了变化，它的确定性增加了，此时的猜测比刚才难度就降低了。通过做减法，就得到了考虑颜色特征时的信息增益0.311。

3.当然也可以用味道作为考虑，混乱程度为0，完全确定。根据味道就可以知道了(当然这是基于数据的)，此时信息增益就是1.

**理解：千万注意这里附加特征之后熵的算法，把特征对应概率放到了外面，里面的项仍然是两种结果的项求和。**



比较下来，味道的信息增益更大，所以，如果基于这个数据集，判断某个果实是不是水果，就会优先考虑"味道"特征。味道提供的有效信息比颜色提供的要多。

理解：熵是用来表述当前选择的状态，信息增益是衡量变化前后熵的变化程度，特征选择是根据信息增益做出的(选最大的)。



### 具体构建过程

**准备的东西都明确之后，就要来具体构建决策树。**

![image-20210727182547596](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727182547.png)



看左侧的流程图，

首先我们就要选择当前最佳特征，因为我们知道决策树其实就相当于一直在问问题嘛，我会问你一系列问题，根据你回答结果的不同会导向一个最终的结果。那所以现在就有了一个问题：第一步要问哪一个问题？这个问题对应的，就是选择当前最佳特征。

选完之后，由这个最佳特征去产生分支，然后看它满不满足分支中的条件，如果满足就结束，没满足就接着循环处理。



然后我们来看右图，具体要怎么选当前最佳特征。

首先计算整体的熵，就是我什么因素都不考虑，我摁猜时熵是多少。然后再去考虑基于特征1的熵，算出它的增益，一直重复算完所有特征。

然后从中选择增益最大的那个特征，就作为当前特征。





---



### 一个完整走一遍流程的样例

![image-20210727184933389](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727184933.png)



附：我也可以算这个，使用我的Anaconda.

![image-20210727201533249](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727201533.png)



最后因为年龄带来的信息增益最大，所以第一级特征我们选择为年龄。



现在我们已经选好当前最佳特征了，按照流程图，接下来要"按照取值产生分支"了。

![image-20210727193152160](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727193152.png)



接下来我们来走一下青少年这个分支。

先判断是否可以中止，看看纯度，即现在里面的数据对应结果是不是都是一样的值(是/否)。目前看还是不纯状态。

然后再看里面的记录数是否小于规定最小记录数，比如我们选择为2，那现在5条，就还不需要。

另外就是循环次数，是不是当前循环次数已经大于我预先规定的值了。

都达不到的话，我们就继续来算。

![image-20210727193556843](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727193556.png)

还是先算青少年的整体熵，然后是加上其他特征后的熵。

最后发现**是否单身**信息增益最高，是0.971，那么下一级就选定这个为分支拆分数据集。



![image-20210727193729787](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727193729.png)



然后现在我们再来看是否满足终止条件。

看左右两个分支，一个全“是”，一个全"否"，说明纯度达标了。我们就没有必要再对分支进行拆分了，左右都是结果一致的了，你去算结果熵也会为0(不存在任何混乱了)。

这样，青少年这条大分支就走完了。



然后再看中年。

![image-20210727194032410](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727194032.png)

发现他纯度就直接ok，那就不用再分了。



最后是老年，这次不行，然后发现“信用等级”划分，最后纯度又为0，整体就结束了。



划分完毕

![image-20210727194203277](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727194203.png)



**最终处理结果**

![image-20210727194144687](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727194144.png)

这就是最终的决策树模型。







### ID3系列算法

刚才介绍的这个生成决策树的方法，就叫做**ID3算法(Iterative Dichotomiser 3 ，迭代树三代)**，发现者是J.Ross quinlan(昆兰)。

**算法核心是信息熵，根据信息增益决定树的节点**

算法存在的问题

* 信息度量不合理：倾向于选择取值多的字段

  理解：？这里所谓取值多应该说的就是记录多吧，是不是跟分数的计算有关，选择完字段之后，会带来新的结果，如果整体趋势差不多的情况下，.... 我也不太懂了。

* 输入类型单一：离散型

  理解：比如我们上面那个样例就是，所有的输入都是离散的，没有连续的

* 不做剪枝，容易过拟合



算法应该算是决策树算法的原型，后续都是对它改进。



因为存在的这些问题，大佬后来又给出了改进。

C4.5与ID3相比的改进：

* 用信息增益率代替信息增益
* 能对连续属性进行离散化，对不完整数据进行处理
* 进行剪枝



再后来的C50 与 C4.5的改进：

* 使用了boosting
* 前修建、后修剪







### CART

此外还有一种常见的决策树分类方法，叫做**CART(Classification and Regression Tree ，分类回归树)。**



**核心是基尼系数(Gini)。**

理解：这就不同于上面那个，那个ID3的核心就是信息熵。

特点

* 分类是二叉树

  理解：上面那个ID3更近似于树，因为子结点未必<=2

* 支持连续值和离散值

* 后剪枝进行修剪

* 支持回归，可以预测连续值





三种算法比较

![image-20210727201019604](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210727201019.png)



可以这样说，ID3是经典的决策树模型，能很好地表达决策树处理问题的思想，是一个demo。

但实际项目中却很少用，后面二者用的更多。





















