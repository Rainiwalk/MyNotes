### 定义

朴素贝叶斯法(NB，Naive Bayes)是基于贝叶斯定理与特征条件独立假设的分类方法。

对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；

然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。



注：朴素贝叶斯法与贝叶斯估计是不同的概念。



现有的分类算法很多，主要分为两种，一是单一分类器，包括决策树、贝叶斯、人工神经网络、支持向量机等；二是集成学习算法，如Bagging和Boosting等。

**贝叶斯**（Bayes)分类算法是一类利用概率统计知识进行分类的算法,如朴素贝叶斯（Naive Bayes）算法。

这些算法主要利用Bayes定理来预测一个未知类别的样本属于各个类别的可能性，选择其中可能性最大的一个类别作为该样本的最终类别。为了计算简单，朴素贝叶斯算法假设条件概率相互独立。



### 朴素贝叶斯法的学习与分类



#### 公式中arg含义

一种对函数求参数(集合)的函数。

arg max f(x): 当f(x)取最大值时，x的取值

arg min f(x)：当f(x)取最小值时，x的取值



#### 求和的消失

这第三步应该只是使用了对立事件的概率公式，但是个人在理解这里是总是感到很混乱，感觉书中概率公式的样子是不是不太对，于是想从头说。

首先来看第一步的这个式子，x已经作为参数传了进去，在等号右侧式子中可以看作一个常量。由于arg和min的存在，我们会尝试不同的y，最后找到使期望风险最小的那个y。在这两个符号右侧，y已经被指定为某一个值，故也可看作**常量**。
右侧的式子就类似于刚学概率论那时给定一个离散的概率分布，求解它的期望一样。表述为对"发生概率*事件结果"的求和，类似E(x) = 1*0.2+2*0.3+3*0.5。最右侧的条件概率就是在求x条件下结果分类是Ck的概率，左侧那个L就是事件结果，我真正的模型预测是y，此处如果y和Ck相等，L就为0，否则为1。因为要走完所有的可能样本点，所以需要一个求和操作。
而因为y只会是所有Ck里的某一个值，那一项对应为0，不参与求和贡献。其余项都是1，再与概率相乘，结果就是那个概率。最终就是那些ck≠y的概率的求和。
到这一步感觉还是比较清晰的。

举个例子，假如我取了一个x，然后在这个题目中，Ck有5种可能.

| (选定x条件下)Ck | 1    | 2    | 3    | 4    | 5    |
| --------------- | ---- | ---- | ---- | ---- | ---- |
| P               | 0.1  | 0.2  | 0.3  | 0.25 | 0.15 |

选定x，y条件来来具体带入一下这个公式。假设y=3

期望 = 0.1x1 + 0.2x1 + 0.3x0 + 0.25x1 + 0.15x1   

然后我们就会发现，结果无非就是将与确定的y不相等的结果发生的概率都加到一起。

虽然具体的数学表示我不确定，但我感觉怎么也应该这么写：

∑P(ck | X=x)   除y=Ck之外  ，

但是直接表述成P(y≠Ck)，y在这里我理解应该是确定的，而对于不同的k对应不同的Ck，两个具体的数相等概率不就是1/0? 那这就和上面的概率思考矛盾了。所以我不怎么明白了。

但是到了第三步，我又感觉到公式应该想表述的意思和我上面的理解应该是差不多的。

他应该就是用了对立，前面是概率求和，表述很麻烦，于是就改写成了1-对立事件的概率。

但这就又有了问题：

* 此时的y表示什么，还是确定量吗？
* 此时的Ck表示什么，赐予k意义的求和公式不是都没了嘛，为什么k还在这里？



最终我还是认为是自己认为的符号含义与书本的不一样，当然可能是我理解不到位。

但那部分公式，我认为就应该是 1 - P(x条件下，y对应的那个事件的概率)。





---

Y = Ck???
