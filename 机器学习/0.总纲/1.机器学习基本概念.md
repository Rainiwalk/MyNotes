### 机器学习方法流程

![image-20210801155017116](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801155017.png)



特征工程：就是在输入数据的基础上做了整理、拓展、合并操作从而得到新的特征。新特征可能和原有特征一样(没有改动)，也可能不一样(做了改动)。

模型训练：使用得到的特征去建模，这是一个很反复的过程

模型部署：如果模型已经很好了，就要去应用，应用之前需要进行一步部署。

最后去应用。





### 基本概念



#### 输入空间和输出空间



**输入空间(Input Space)**：输入的所有可能取值的集合

**输出空间(Output Space)**：输出的所有可能取值的集合



举个例子.

![image-20210801160332268](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801160332.png)

解释：左边这个表，所有父亲的身高就是输入空间，所有儿子的身高就是输出空间。

右边这个表，是根据一个人诸多相关信息作为一个输入，然后对应生还还是遇难的结果。这里这个人的信息就不是单纯一个值了，而是一个多维数据，即一个特征向量。此时输入空间也不再是一维，而是多维空间。





**二者取值范围**

* 输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间

  欧式空间的概念看似新颖，其实没什么，就和平常函数的定义域是R一样的存在，只是因为维度上升了，所以这个范围不再是线性的R，变成了整个欧氏空间

* 输入空间与输出空间可以是连续值集合，也可以是离散值集合

  输出是年龄的值，输出是少年、青年、老年

* 输入空间与输出空间可以是同一个空间，也可以是不同的空间

* 通常输出空间远远小于输入空间









#### 特征空间



**特征(Feature)**：即属性。每个输入实例的各个组成部分(属性)称作原始特征，基于原始特征还可以拓展出更多的衍生特征。其实就是一个事物具有的一个小属性。

**特征向量(Feature Vector)**：由多个特征组成的集合，称作特征向量。很明显，这里多个特征是属于一个个体的。

**每个具体的输入是一个实例(instance)，通常由特征向量(feature vector)表示。**



**特征空间(Feature Space)**：所有特征向量可能取值的集合称作特征空间。





举例

![image-20210801161836244](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801161836.png)

每个部分都是特征，这里并非取了所有，而是选择了三个原始特征，组成了我需要的特征向量



**特点**

* 特征向量每一维对应于一个特征

* 特征空间可以和输入空间相同，也可以不同

  如果原始特征状态就很好，那就直接用；否则，我就对它进行加工组合，从原始数据抽取出特征向量，此时两个空间就不同了

* 需要将实例从输入空间映射到特征空间

  一般意义上来说是有这一步的，只是前后相同，可能前后不同

* 模型实际上都是定义在特征空间上的



理解：这一点现在我也有了一点体会了，多亏了吴恩达老师的课程。有些时候，我们直接用输入的一些特征就很好，但更多时候，我们会重新对特征进行定义，让它成为包含输入再加上一些新东西的状态，却更能方便我们构建模型。







#### 假设空间

假设空间(Hypothesis Space)：由输入空间到输出空间的映射的集合，称作假设空间。



![image-20210801162631658](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801162631.png)

附：统计学习方法和西瓜书老师比较推崇！

理解：就是我们认为由输入到输出可以找到各种各样的拟合模型，把他们都算上就是假设空间，而我们只需要从里面拿一个最好的。至于怎么找到这个最好的，就是我们要研究的问题。





一个很好的举例.

![image-20210801163506072](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801163506.png)

首先左侧是一个商品购买记录，我们要基于这个进行建模。

这里先假设没有新映射，所以输入=特征。

可以知道输入记录中所有可能出现的结果将是2*3 = 6种，如中间这个表格。

每种组合会对应买与不买的两种可能。



那么，列出的这个表叫假设空间吗？



我们接着往下看。

其实假设空间应该是：对于每一种可能的输入，**都能**找到**一个**映射，对应了输入空间中某个输出。



![image-20210801164156371](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801164156.png)

原本左侧是上面的12行表格，就是所有的可能，但是只有抽取出部分，使得每个输入都能找到也只能找到一个对应。这才是一个假设h。这就是一种映射的方式。

但我们想一下就知道，不止这一种假设。也许Female+High -> True。

会有很多种假设的可能。

我们算一下。

![image-20210801164359087](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801164359.png)

公式中：M为所有可能结果的个数，这里就是2，头顶是不同特征数量的连乘。最后加1。



把它们都列出来...

![image-20210801164715808](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801164715.png)



很多对吧，这个所有的可能组成的才是假设空间。

我们的目的就是从这里面找到对数据最拟合的那个假设，然后把这个假设作为模型对新数据进行预测。



懂啦！！！









监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数(decision function) Y = f(X) 表示，随具体学习方法而定。对具体的输入进行相应的输出预测时，写作 P(y|x)或y = f(x)。

理解：后面具体化后，无非把随机变量替换成了具体的某一个数据。
