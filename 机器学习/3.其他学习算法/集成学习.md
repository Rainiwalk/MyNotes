### 定义

集成学习：针对同一数据集，训练多种学习器(不同算法)，使用不同的组合方式把这些学习器组合起来来更好地解决同一问题(比单一学习器更好)。



常见有3种

* Bagging
  * 有放回抽样构建多个子集
  * 训练多个分类器
  * 最终结果由各分类器结果投票得出
  * 实现非常简单，很好理解
* Boosting
  * 重复使用一类学习器来修改训练集
  * 每次训练后根据结果调整样本的权重
  * 每个学习器加权后的线性组合即为最终结果
* Stacking
  * 由两级组成，第一级为初级学习器，第二级为高级学习器
  * 第一级学习器的输出作为第二级学习器的输入







### Bagging

![image-20210801104653374](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801104653.png)

补充：通过有放回抽样的方式产生的多个子样本，然后子样本取应用不同的机器学习算法，得到不同的模型，最后通过下面的公式(投票公式)把模型都叠加起来。



举个例子，假设这是一个二分类问题。

![image-20210801105314860](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801105314.png)

首先我有放回抽样，最后建立起了3个正确率在60%的模型，把它们组成一个新的模型。

对于外来数据，我的每个模型都有预测正确与错误两个结果，用0表示预测错误，1表示预测正确。

3个模型，组建成表格就是2^3 = 8种可能。

如果是 0 0 0 ，那就表示对于这个数据，3个模型都预测错误了。这样的概率是0.064。

那对于这样一个表格，最终输出的结果是什么的，这就要看投票模型 - 很简单，最终结果会采用大家出现次数最多的那个答案，即"少数服从多数"。

所以对于前面4种组合，尽管由预测正确的单个机器，但我最终信服的还是那个错误结果。

最后，发现出错总概率是0.352，正确就是0.648。



而64.8% > 60% ， 这也就说明了新模型比原模型的优势。





那我们就可以想一下，既然增加模型的数量来进行Bagging可以增加预测正确率，那么能不能使用更多的模型来提高呢？

理论上是这样(纯数学计算).

![image-20210801110052320](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801110052.png)



但实际上这是达不到的，

**因为：模型之间未必独立**

我们上面的分析用了这样一个假设 --- 模型与模型之间独立，所以我们才可以计算这样的概率，事实上不可能独立，可能模型算法是一样的，可能使用的样本也不是完全独立的。

现实很难达到。

**但Bagging还是有用的，使用它确实是很容易得到一个比原有模型更优的模型的，只是效率增加特别多的事还是别想了，很难达到。**







### Boosting

类似于串联的结构

![image-20210801110627780](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801110627.png)



思路：首先有一个训练样本，基于机器学习算法可以得到一个模型1。之后我把这个模型1去作用于前面那个训练样本，就会发现有的预测到了，有的还是错误。那我就把错误的记录权重增大，再把它做成一个数据集，也就是训练样本1。然后再用那个机器学习算法，就可以得到模型2。之后不停迭代。

从公式也可以看到，每个模型h(x)都有个一个权重参数，然后把它们叠加起来



理解：就和平常刷题一样，做了题有对错，然后接下来我就会更加去做那些出错的题目。





Boosting有很多种方法，我们这里看到的是**AdaBoost**。



比如现在有一个二分类的数据集，分类的标记都标出来了。

![image-20210801111338286](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801111338.png)

5个正例，5个负例.



首先我应用算法生成了一个分类器，这个分类器就会把数据分成两部分。这就是我们的第一个模型。

![image-20210801111456271](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801111456.png)

但我们可以看到，有三个+理应在左边，但分类器却把它分到了右边。



对于这些错误的数据，我们就要给他更高的权重(让错误的+更大)。

![image-20210801111615486](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801111615.png)

然后把改动后的样本做出基础，再去训练模型，就得到了第二个训练模型。



但是我们发现左侧又出现了问题，于是还要继续迭代....

![image-20210801111852197](https://raw.githubusercontent.com/Rainiwalk/Rain_image/main/2021/20210801111852.png)



最后我们把过程中总共得到的3个模型叠加起来，就是最终模型的结果。

可以看到有了很大进步。





