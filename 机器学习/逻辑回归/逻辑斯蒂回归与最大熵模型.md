### 前言

**逻辑斯谛回归(Logistic Regression)**是统计学习中的经典**分类方法**。

**最大熵**是概率模型学习的一个准则，将其推广到分类问题得到**最大熵模型(maximum entropy model)**。

逻辑斯蒂回归模型与最大熵模型都属于**对数线性模型**。



理解：看来应该是两个东西，为什么会放到一章来讲呢？一定有其道理。



---



### 逻辑斯谛回归模型



咱就不求甚解吧，有问题确实可以都放在这里。找时间总结总结。

但是落实到写代码上，应该也不难吧。



-



老师讲解说：在这里，重点是**二项逻辑斯谛回归模型**，后面的多项倒是不做重点，因为常用逻辑斯谛回归解决二分类比较多(而且我觉得多项也无非是个推广，有了二也就不怕多了)。

这个在工作中是很常用的，比如淘宝的推荐算法内部用的都是逻辑斯谛回归模型，好处有

* 速度快

  整个推理训练的过程是比较快的。后续学到SVM，就知道什么是慢了。

* 性能不错

  和感知机差不多，但是感知机在工业中很少被用，因为缺陷太多了



















### 最大熵模型



怎么说？

之前是连续型的分布，我根本就没法比较，我自己固定了它是几个分类，然后去计算概率，比较抉择。

现在轮到离散型了，它本身就是分类，我把所有可能穷尽去比较就可以。所以现在关键就在于找到那个分布了。能求概率就可以结束了。

然后如果什么条件都没有，那分布就是没法确定的，但我可以有一个暂时的假定，就是他们是均匀分布等概率，所以每一个概率我也就都知道了。

这个是很万能的，所谓最大熵似乎就是这样。

就是在现有条件你虽然能给出一些约束，但是分布仍然可能有很多种，此时就极简化的考虑，对每个约束条件，能认为均匀的就均匀来看。

哈哈，也没那么难。如果我考试不会，去蒙可能就会这么干。









